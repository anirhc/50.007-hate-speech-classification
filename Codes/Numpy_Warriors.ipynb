{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Warriors\n",
    "**Group Members:**  \n",
    "- 1005501 Harikrishnan Chalapathy Anirudh\n",
    "- 1005802 Swastik Majumdar\n",
    "- 1005374 Nguyen Thai Huy\n",
    "- 1005147 Hayden Ang Wei En \n",
    "- 1005265 Win Tun Kyaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17179</th>\n",
       "      <td>17180</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17180</th>\n",
       "      <td>17181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17181</th>\n",
       "      <td>17182</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17182</th>\n",
       "      <td>17183</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17183</th>\n",
       "      <td>17184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17184 rows × 5002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label    0    1    2    3    4    5    6    7  ...  4990  4991  \\\n",
       "0          1      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1          2      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2          3      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3          4      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4          5      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "...      ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "17179  17180      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "17180  17181      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "17181  17182      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "17182  17183      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "17183  17184      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "       4992  4993  4994  4995  4996  4997  4998  4999  \n",
       "0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "17179   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "17180   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "17181   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "17182   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "17183   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[17184 rows x 5002 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_tfidf_features.csv\")\n",
    "df1 = pd.read_csv(\"test_tfidf_features.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "      <td>17184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8592.500000</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4960.737848</td>\n",
       "      <td>0.485702</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.019532</td>\n",
       "      <td>0.024741</td>\n",
       "      <td>0.012334</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>0.005065</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010215</td>\n",
       "      <td>0.013178</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.008536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4296.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8592.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12888.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17184.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.676327</td>\n",
       "      <td>0.560830</td>\n",
       "      <td>0.958430</td>\n",
       "      <td>0.646740</td>\n",
       "      <td>0.532789</td>\n",
       "      <td>0.437760</td>\n",
       "      <td>0.435835</td>\n",
       "      <td>0.536746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611122</td>\n",
       "      <td>0.540809</td>\n",
       "      <td>0.566613</td>\n",
       "      <td>0.592170</td>\n",
       "      <td>0.617341</td>\n",
       "      <td>0.850605</td>\n",
       "      <td>0.484908</td>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.430031</td>\n",
       "      <td>0.528556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 5002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         label             0             1             2  \\\n",
       "count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n",
       "mean    8592.500000      0.381227      0.000150      0.001066      0.001532   \n",
       "std     4960.737848      0.485702      0.008297      0.019532      0.024741   \n",
       "min        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     4296.750000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     8592.500000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    12888.250000      1.000000      0.000000      0.000000      0.000000   \n",
       "max    17184.000000      1.000000      0.676327      0.560830      0.958430   \n",
       "\n",
       "                  3             4             5             6             7  \\\n",
       "count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n",
       "mean       0.000369      0.000140      0.000066      0.000270      0.000483   \n",
       "std        0.012334      0.008276      0.005065      0.009907      0.013106   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.646740      0.532789      0.437760      0.435835      0.536746   \n",
       "\n",
       "       ...          4990          4991          4992          4993  \\\n",
       "count  ...  17184.000000  17184.000000  17184.000000  17184.000000   \n",
       "mean   ...      0.000202      0.000429      0.000286      0.000075   \n",
       "std    ...      0.010215      0.013178      0.011378      0.005866   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      0.611122      0.540809      0.566613      0.592170   \n",
       "\n",
       "               4994          4995          4996          4997          4998  \\\n",
       "count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n",
       "mean       0.000260      0.000709      0.000257      0.000121      0.000308   \n",
       "std        0.010864      0.017641      0.010246      0.006529      0.010526   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.617341      0.850605      0.484908      0.398105      0.430031   \n",
       "\n",
       "               4999  \n",
       "count  17184.000000  \n",
       "mean       0.000159  \n",
       "std        0.008536  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        0.528556  \n",
       "\n",
       "[8 rows x 5002 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of target :  (17184,)\n",
      "Shape of features :  (17184, 5000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = df['label'].to_numpy()\n",
    "train_features = df.drop(['id','label'], axis = 1).to_numpy()\n",
    "test_features = df1.drop(['id'], axis = 1).to_numpy()\n",
    "print(\"Shape of target : \", train_target.shape)\n",
    "print(\"Shape of features : \", train_features.shape)\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_pred):\n",
    "    loss = -np.mean(y*(np.log(y_pred)) - (1-y)*np.log(1-y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y, y_pred): \n",
    "    # X is the input, y is the target value, y_pred is the prediction.\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    dw = (1/m)*np.dot(X.T, (y_pred - y))\n",
    "    db = (1/m)*np.sum((y_pred - y)) \n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, bs, epochs, lr):\n",
    "    # X is the input, y is the target value, y_pred is the prediction.\n",
    "    # bs is the batch size, epochs is the number of iterations and lr is the learning rate.\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize weights and bias to zeros.\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    \n",
    "    # Reshaping y.\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    # Initialize an empty list to store the losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m-1)//bs + 1):\n",
    "            \n",
    "            # Define the batches.\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Calculate the prediction.\n",
    "            y_pred = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            # Getting the gradients of loss with respect to the parameters.\n",
    "            dw, db = gradients(xb, yb, y_pred)\n",
    "            \n",
    "            # Update the parameters.\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "        \n",
    "    # returning weights, bias and losses.\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w,b):\n",
    "    \n",
    "    # Calculating predictions.\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    \n",
    "    # Empty List to store predictions.\n",
    "    pred_class = []\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    \n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, l = train(train_features, train_target, bs = 100, epochs = 1000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred):\n",
    "    accuracy = np.sum(y == y_pred) / len(y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706762104283054\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(train_target, predict(train_features, w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = pd.DataFrame(predict(test_features, w ,b))\n",
    "#pd.concat([predictions,df1['id']],axis = 1).to_csv(\"LogRed_Prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Apply dimension reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(\"train_tfidf_features.csv\")\n",
    "test_dataset = pd.read_csv(\"test_tfidf_features.csv\")\n",
    "\n",
    "x = dataset.drop([\"label\",\"id\"],axis = 1).to_numpy()\n",
    "y = dataset[\"label\"].to_numpy()\n",
    "\n",
    "x_test = test_dataset.drop([\"id\"],axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing dimension of training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = StandardScaler()\n",
    "scaling.fit(x)\n",
    "Scaled_data1 = scaling.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17184, 100)\n"
     ]
    }
   ],
   "source": [
    "pca_100 = PCA(n_components = 100)\n",
    "pca_100.fit(Scaled_data1)\n",
    "x_100 = pca_100.transform(Scaled_data1)\n",
    "print(x_100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17184, 500)\n"
     ]
    }
   ],
   "source": [
    "pca_500 = PCA(n_components = 500)\n",
    "pca_500.fit(Scaled_data1)\n",
    "x_500 = pca_500.transform(Scaled_data1)\n",
    "print(x_500.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17184, 1000)\n"
     ]
    }
   ],
   "source": [
    "pca_1000 = PCA(n_components = 1000)\n",
    "pca_1000.fit(Scaled_data1)\n",
    "x_1000 = pca_1000.transform(Scaled_data1)\n",
    "print(x_1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17184, 2000)\n"
     ]
    }
   ],
   "source": [
    "pca_2000 = PCA(n_components = 2000)\n",
    "pca_2000.fit(Scaled_data1)\n",
    "x_2000 = pca_2000.transform(Scaled_data1)\n",
    "print(x_2000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing dimension of test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = StandardScaler()\n",
    "scaling.fit(x_test)\n",
    "Scaled_data = scaling.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4296, 100)\n"
     ]
    }
   ],
   "source": [
    "pca_100 = PCA(n_components = 100)\n",
    "pca_100.fit(Scaled_data)\n",
    "x_test_100 = pca_100.transform(Scaled_data)\n",
    "print(x_test_100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4296, 500)\n"
     ]
    }
   ],
   "source": [
    "pca_500 = PCA(n_components = 500)\n",
    "pca_500.fit(Scaled_data)\n",
    "x_test_500 = pca_500.transform(Scaled_data)\n",
    "print(x_test_500.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4296, 1000)\n"
     ]
    }
   ],
   "source": [
    "pca_1000 = PCA(n_components = 1000)\n",
    "pca_1000.fit(Scaled_data)\n",
    "x_test_1000 = pca_1000.transform(Scaled_data)\n",
    "print(x_test_1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4296, 2000)\n"
     ]
    }
   ],
   "source": [
    "pca_2000 = PCA(n_components = 2000)\n",
    "pca_2000.fit(Scaled_data)\n",
    "x_test_2000 = pca_2000.transform(Scaled_data)\n",
    "print(x_test_2000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2000 components training f1 score\n",
    "neigh_2000 = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh_2000.fit(x_2000, y)\n",
    "\n",
    "#1000 components training f1 score\n",
    "neigh_1000 = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh_1000.fit(x_1000, y)\n",
    "\n",
    "#500 components training f1 score\n",
    "neigh_500 = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh_500.fit(x_500, y)\n",
    "\n",
    "#100 components training f1 score\n",
    "neigh_100 = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh_100.fit(x_100, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 50.6638720035553 seconds ---\n",
      "--- 1.2869670391082764 seconds ---\n",
      "--- 0.7224512100219727 seconds ---\n",
      "--- 0.2800109386444092 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time_2000 = time.time()\n",
    "predictions_2000 = neigh_2000.predict(x_test_2000)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time_2000))\n",
    "start_time_1000 = time.time()\n",
    "predictions_1000 = neigh_1000.predict(x_test_1000)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time_1000))\n",
    "start_time_500 = time.time()\n",
    "predictions_500 = neigh_500.predict(x_test_500)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time_500))\n",
    "start_time_100 = time.time()\n",
    "predictions_100 = neigh_100.predict(x_test_100)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([pd.DataFrame(predictions_2000),test_dataset['id']],axis = 1).to_csv(\"knn_predictions_2000.csv\")\n",
    "# pd.concat([pd.DataFrame(predictions_1000),test_dataset['id']],axis = 1).to_csv(\"knn_predictions_1000.csv\")\n",
    "# pd.concat([pd.DataFrame(predictions_500),test_dataset['id']],axis = 1).to_csv(\"knn_predictions_500.csv\")\n",
    "# pd.concat([pd.DataFrame(predictions_100),test_dataset['id']],axis = 1).to_csv(\"knn_predictions_100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **F1 score for 100: 0.52971**\n",
    "- **F1 score for 500: 0.49234**\n",
    "- **F1 score for 1000: 0.43323**\n",
    "- **F1 score for 2000 : 0.33998**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the reduced components\n",
    "- **Time** - By using the time module, we were able to record the time taken for each prediction.  We obtained 53.19764566421509 seconds for n_components = 2000, 1.2955138683319092 seconds for n_components = 1000, 0.733565092086792 seconds for n_components = 500 and 0.30568814277648926 seconds for n_components = 100. Thus, we can see that, as we reduce the dimension, we can also reduce the time required to train the machine learning model. This is mainly because the lower the dimension, the less computationally expensive it is. Thus, training takes less time while retaining our accuracies, which increases our efficiency.\n",
    "- **F1 score** - We also noticed that the F1 score reduced as the dimensions increased. We find that for KNN, reduced dimensions work better for model accuracy, and this further supports our motive for using PCA to reduce the dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Try other machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data pre-processing***\n",
    "-   **SMOTE (Synthetic Minority Oversampling Technique)**  \n",
    "While performing exploratory data analysis, we found that our dataset is highly skewed to one side. We have more 0's than we have 1's. As a result, while predicting unseen data, our models are more likely to choose 0 as compared to 1. To solve this issue, we tried \"oversampling\". In this technique, we synthetically generate new data items from the minority class. We keep generating new items until the ratio of the classes comes out to be 1:1. Now, the model trains on equal number of 1's as it does to 0's. Thus, whenever it predicts on unseen data, it is not biased to choose either class.\n",
    "-   **StandardScaler**  \n",
    "Using StandardScaler, we will standardize the features by scaling to unit variance. Several machine learning estimators rely on standardization of datasets. If the individual features do not follow a standard normally distributed distribution, they may behave poorly.\n",
    "-   **MinMaxScaler**  \n",
    "Using MinMaxScaler, we will scale each feature individually so that it fits within the training set's range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(\"train_tfidf_features.csv\")\n",
    "test_dataset = pd.read_csv(\"test_tfidf_features.csv\")\n",
    "\n",
    "final_test_x = test_dataset.drop([\"id\"],axis=1)\n",
    "final_test_id = test_dataset[\"id\"]\n",
    "\n",
    "x_original = dataset.drop([\"label\",\"id\"],axis = 1)\n",
    "y_original = dataset[\"label\"]\n",
    "\n",
    "#Balance dataset\n",
    "x, y = SMOTE().fit_resample(x_original, y_original)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05)\n",
    "\n",
    "st_scaler= StandardScaler()\n",
    "st_scaler2= StandardScaler()\n",
    "mm_scaler = MinMaxScaler()\n",
    "\n",
    "## STD Scale\n",
    "x_train_st = st_scaler.fit_transform(x_train) \n",
    "x_test_st = st_scaler.transform(x_test) \n",
    "x_st = st_scaler2.fit_transform(x) \n",
    "final_test_st = st_scaler2.transform(final_test_x) \n",
    "\n",
    "## MM Scale\n",
    "x_train_mm = mm_scaler.fit_transform(x_train)\n",
    "x_test_mm = mm_scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(y_original.value_counts(), autopct=\"%0.2f\")\n",
    "plt.title(\"Distribution of classes before oversampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD3CAYAAADlsBq6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcHElEQVR4nO3deXxcVd3H8c9J0iVbU9oCbUPpUApFrKzKJgiPFSiE9VFBUGhZpIVHFEVhfES8sg4vpcCjrFVks7JUsOCwCYoKtkIrRVCB1jICbSi0pZOkTdMmOc8f54ROJ8udJDNz5t77e79efXUyy53vzD3znXPvJHOV1hohhOhLmesAQojSJ0UhhPAlRSGE8CVFIYTwJUUhhPAlRSGE8JWXolBK3aaU+n6elrWzUqpFKVVuf35OKXVuPpZtl/eEUmpGvpbXj/u9Sim1Rin1Xj9vl9fH74JS6tNKqWV2vZ7kOk8YKKW0UmqyPZ23119vKnIIlAJ2BNqBDuCfwD3AHVrrTgCt9exc7swu61yt9TO9XUdr/TZQk8vycrg/D5istf5KxvKPycey+5ljZ+BiYKLW+v1i338JuAL4qdb6JjCDHNhNa73cbaxwyPX1Nxi5ziiO11rXAhOBBHAp8PN8h1FK+RZXQO0MrI1oSYAZN//Ix4JcjpEQj09/Wus+/wEp4HNZ5x0AdAJT7c93AVfZ02OA3wLrgXXAnzGFdK+9TSvQAlwCxAANnAO8Dfwp47wKu7zngGuBF4EmYAEwyl52BPBuT3mB6cBmYIu9v1cylneuPV0GXAb8B3gfM1Oqs5d15Zhhs60BvtfH81Rnb/+BXd5ldvmfs4+50+a4q5fbnwgstY/x38D0HvLuCvweWGvz/BIYmbGMS4GVQDPwBjAtY30ttsteDczJuM1BwF/s+noFOCLjspnACru8t4Av95L9AGChXUYj8FNgqL3s31nrfaF9XjfYn0+11zvOPv71Ns9eWev0UuDvQBt2bGRlOAR4CUjb/w+x558KLM667jeBR+3pYcCP7TpeDdwGVGaOL3vf72HGcI/j214/bh9vM2bmfXLWc/kCcIO97QqbeSbwDmb8zci4/l02y+/s8v6ImZF2Xa4xs+Wu616Vlfliu8xG4KyM240GHsOMhZeAq4DnfXtgIEVhz38bOL+HoNfaBzjE/jsMUD0ti60vxnuAaqCSnotiJTDVXufXwH1+RWFPe13Xzbj8Oba+8M4GlgOTMJs7DwP3ZmWba3PtjRmkH+vleboHU2K19rZvAuf0lrOHF1oaOBJTLvXAHj3knWyvMwzYHlOsN9rLpmAG3PiM/Lva0wuBM+zpGuAge7oeUzrH2vs90v68vX2um4Ap9rrjgI/3kn9/TOFU2Pv9F3BRb2OIjEFuf94XM6gPBMox5ZwChmXcfikwAfsizrr/UcCHwBk2w2n259FAFeaFtlvG9V8CvmRP3wA8apdRi3kRXZux3tqB6+xzXknf4/uLwHj7XJ6KKcNxGUXRDpxlH+NVmNfQzXbZR9mcNRmvqWbgM/bym8h4QdN3UbRjNveG2HW7EdjOXn6//VcF7IkZMwUtikXYd9isoFdgXjCT/ZbF1hfjpB7OyyyKRMble2JmCuUMviieBS7IuGwKZgbSNeA1sFPG5S9iB1jWMsttpj0zzpsFPJdjUdwO3NDLZR/l7eGyk4CXM0rkfcwMZkjW9f4E/BAYk3X+pdhizDjvKcwLtRrzzvd5enhx+oyZi4BH+lEUtwJXZi3jDeDwjNuf3cf9nQG8mHXeQmCmPX0fcLk9vRvmBVgFKMyLedeM2x0MvJWx3jYDwzMu73V895BrKXCiPT0TWJZx2Sfs87BjxnlrgX0yXlP3Z1xWg9lHOCH7OaR7UbSSMeuy4+IgO063YMvfXpbTjGIwn3rUY6Ze2X6EeZd+Wim1QikVz2FZ7/Tj8v9gmnJMTin7Nt4uL3PZFZidt10yP6XYSM87WsfYTNnLqs8xxwTMlLVPSqkdlVL3K6VWKqWaMC+AMQDa7Bi8CFOO79vrjbc3PQfYHXhdKfWSUuo4e/5E4ItKqfVd/4BDMe+CGzDvirOBRqVUUim1Ry+5dldK/VYp9Z7NdQ39Wz8TgYuzckzArJ8ufY2R7PUI2z7/8zCzDIDTgd9orTdiZk5VwJKM+33Snt/lA631poyfex3fSqkzlVJLM5Y1lW2fh9UZp1sBtNbZ52WOr48es9a6BfN6y3xOerNWa92e8XPXuN0eM74zn0u/1x4wwI9HlVKfwqyE57Mv01o3a60v1lpPAk4AvqWUmtZ1cS+L7O38LhMyTu+MacU1mHeDqoxc5Wy7kv2WuwozSDOX3c62KzQXa2ym7GWtzPH272D2P/i5BvOYPqG1HgF8BfOuCIDWep7W+lCbQ2OmzGitl2mtTwN2sOfNV0pV2/u9V2s9MuNftdY6YW/3lNb6SMxmx+uYzbCe3Gov383m+t/MXDk+/quzclRprX+VcZ2+1mX2eoRtn//fAdsrpfbBFMY8e/4azIvz4xn3W6e1znyxbnO/vY1vpdREzPPzNWC01nok8Br9ex6yfTTulVI1mM2jVYNY3geY8b1TT/fRl34VhVJqhH03uh8zpX+1h+scp5SarJRSmO3uDszOLDAvwEn9uU/rK0qpPZVSVZip33ytdQdmP8BwpVSDUmoIZgfisIzbrQZiSqneHuevgG8qpXaxK+Ia4IGsNvZlszwIXK2UqrWD5luYd/xc/Bw4yw64MqVUfS/v3rWYHYBppVQ98J2uC5RSU5RSn1VKDQM2sXUHKkqpryilttfm4+z19iadNt/xSqmjlVLlSqnhSqkjlFI72dnLibZQ2uz9dq3HnnI1AS029/k+jzd7HMwFZiulDlRGtV2ntT7L6fI4sLtS6nSlVIVS6lTMJupvAbTWW4CHMLOBUZjiwD4fc4EblFI72OeqXil1dG931Mf4rsaUygf2emdhZhSDcaxS6lCl1FDgSmCR1jqnGUBP7Dh9GPCUUlV2XZ2Zy21zLYrHlFLNmOb/HjAHs1OmJ7sBz7B1D/ctWus/2MuuBS6zU7Nv53jfYPY234XZDBgOfB1Aa50GLgB+hnn32IDZ49vlIfv/WqXU33pY7p122X/C7NXfBFzYj1yZLrT3vwIz05pnl+9La/0i5vm8ATP4/kj3d0gw+xn2s9dJYlZ6l2GYj67XYJ6nHYDv2sumA/9QSrVgdop9SWvdagfdiZgZwAeY9fsdzLgow5TdKsyU93B6L4BvY6b0zZgX3gM+D9kD7rbj4BSt9WLgq5hPSz7ETO1n+izjI1rrtZhPTS7GbOdfAhyntV6TcbV5mP03D2W9EVxq72+R3Wx6BrOvqjc9jm+t9T+B6+15qzH7IF7I9TH0Yh7wA8zzvz9mBjlYX8N8Qtf1Kc6vMG8EferaWyuEKCFKqbswO8AvK/D9XAeM1VrP6Ot68rceQkSIUmoPpdRedhPvAMyO7kf8bhfd3zQTIppqMZsb4zGbSNdjPu7tk2x6CCF8yaaHEMKXFIUQwpcUhRDClxSFEMKXFIUQwpcUhRDClxSFEMKXFIUQwpcUhRDClxSFEMKXFIUQwpcUhRDClxSFEMKXFIUQwpcUhRDClxSFEMKXFIUQwpcUhRDClxSFEMKXFIUQwpcUhRDClxSFEMKXFIUQwpccACjkYvFkOeY4pOMwB33J/n8sUIMZC0PYOiY6MEe+3oI5Juv7mOOQNvbwf2Mq0eB7/EoRXHIAoBCJxZOVwN6YA9ruD3wS+BjFeUP4D7AYWNL1L5VoWFuE+xVFIEURYLF4cirmKOPFLoVcZZbHQuD5VKKhve+biFIkRREgsXhyCKYYjrf/dnGbqN/WA08AjwGPpxINabdxRK6kKEpcLJ4cCRwLnABMB+qcBsqfLcCfgUeBR1OJhrcc5xF9kKIoQbF4sgxTDrOBoymtzYlCWQrcDtyXSjS0OM4iskhRlJBYPDkGOBeYBcTcpnGmCbgXuDWVaPiH6zDCkKIoAbF48hDgAuALwDDHcUrJn4BbgIdTiYYtrsNEmRSFI7F4UgGnAHFgH7dpSt57wE+BG1OJhg2uw0SRFIUDsXjyKOBaYD/XWQJmNXAVcLvMMIpLiqKIYvHkAZiC+KzrLAG3Avg+8KtUokEGcBFIURRBLJ6cAlwNfN51lpB5BfhuKtHwhOsgYSdFUUCxeHIEcB3wVaDccZwwew44P5VoeN11kLCSoiiQWDx5NDAXmOA6S0RsAi4H5qQSDR2uw4SNFEWe2VnEHOAc11kiahFwlswu8kuKIo9kFlEyumYX16cSDZ2uw4SBFEUeyCyiZC0CZqYSDW+4DhJ0UhSDFIsn9wYWABNdZxE92gTMSiUa7nEdJMikKAYhFk9+AbgLqHYcRfi7HrhENkUGRopiAOyvX3uYX/pRbtOIfngCOE2+B6P/pCj6KRZPVmP+uvFk11nEgLwBnJBKNLzpOkiQyLdw90MsnowBf0FKIsimAH+NxZPTXQcJEimKHMXiyU8DLwF7uc4iBm0k8NtYPPkN10GCQooiB7F48rPAU8AY11lE3pQDN8biye+7DhIEUhQ+YvHkMUAS+WQjrK6IxZNXuw5R6mRnZh9i8eTxwHxgqOssouDmpBINF7sOUapkRtELO5OQkoiOb8XiyYTrEKVKiqIHdp/Ew0hJRM2lsXjyB65DlCLZ9Mhiv+j2d0CV6yzCmUtSiYYfuQ5RSqQoMsTiyYmYj0C3d51FOKWBz6cSDY+4DlIqpCgs+xuXL2AO8itEC3BIKtHwqusgpUCKgo/+duMhQvqdlu/eejZlQyuhrAxVVs64GTfS0drMmgXX0d60mooROzLmpDjlw2u63bbl1WdJL7wfgLqDv0TNJ6YB0PbectYmb0C3b6Zy10+y3bTzUCp0f/aSAj6VSjSscR3ENdmZaVxOSEuiy46nXcP4s37CuBk3AtC06CGGx/am/ry5DI/tTdOih7rdpqO1mfQL8xh7xhzGnnkD6Rfm0bHJHO1v3dM3M3r6hYw/7w62rFvFphVLivlwiiUGzLcHh460yBdFLJ78byBye7o3Lv8r1VPN7KB66jQ2LlvU7Tqb3vobw2P7Ul5ZS/nwGobH9mXTiiW0t6yjs62VYfV7oJSiZupne7x9SBwO3OQ6hGuRLopYPLkXcA9h/1NxpXj/wctpvOsbNC99EoCODeupqBkFQHn1dnRsWN/tZu3NaykfsfW31strR9PevJaO5rVU1I7e5vyOlrWFfQxunR+LJ2e7DuFSFI6S3aNYPFkDPEIEfjV77Jevo6J2DB0b1rP6gcsYMnqnbS5XSoW8KfPi/2Lx5N9SiYYXXQdxIcozih8Bk1yHKIaKWjMrKK8eSdXuB9O26k3Kq0fS3rIOgPaWdZRVj+zhdqPpaNq6H69rJtE1s8g8v7xmdLfbh8wQ4BexeDKSB5GOZFHE4slpwCzXOYqhc/MmOts2fnR601svM3T7iVRNPpANrz0LwIbXnqVq8oHdbjt8l/1oTb1Mx6YWOja10Jp6meG77EdFzSjKhlXStvJ1tNa0vPZ7qnbrfvsQ2hPzzWaRE7mPR+0mx6uYPdqht2X9e3zw8FXmh85Oqvc8nLpDTqWjtYk1CxK0N31AxYgdGHNinPLKWtoal9Gy9AlGH/N1AFr+/jTpheYTkbqDT6FmryMBaGtcxtrH7cejk/Znu8/NDuPHoz3pAA5OJRpech2kmKJYFLcCkd4xJQbtn8B+qURDm+sgxRKpTY8obXKIgorcJkhkZhRR2+QQBRepTZAozSiuRkpC5E85cGcsnozEUeojURSxeHIScL7rHCJ0pgIzXIcohkgUBXAl5nNwIfLNi8WTw12HKLTQF4U9NuhprnOI0JoA/I/rEIUW+qIAriXsf8shXPvfWDxZ5zpEIYW6KGLx5OHAMa5ziNAbBVziOkQhhbooAPlWZVEsF8XiybGuQxRKaIsiFk+eBBzkOoeIjCrMFyCFUmiLArjUdQAROWfH4slQHnYylEURiyf3RWYToviGAee4DlEIoSwK4ALXAURkzYrFk6F7XYXuAdmPqU53nUNE1i6E8JO20BUFMBM5ypdwK3Qz2lD99ag9Pse/gCmus4hI6wQmpxINb7kOki9hm1FMQ0pCuFdGyL4cKWxFEaqVIwLt7Fg8OdR1iHwJTVHE4skqoMF1DiGsMcB/uQ6RL6EpCuAoIPR/7isC5QTXAfIlTEURmpUiQuN41wHyJRSfethfcGkEdnCdRYgs+6USDS+7DjFYYZlRHIiUhChNoZjphqUoQrEyRCiFYmxKUQhRWPvF4sl61yEGK/BFYb9he0/XOYToQ+B3aga+KIDDXAcQwkfgx2gYimJ/1wGE8PFJ1wEGKwxFEfiVIEJvt1g8OcJ1iMEIdFHYw7nt4zqHED4UsK/rEIMR6KIAPgZUug4hRA4CvYkc9KII9JMvIiXQY1WKQojiCPS+tKAXRaCffBEpgd6hGfSi+LjrAELkSGH2qQVSYIsiFk9WA4FtaBFJ410HGKjAFgUBftJFZAV2zAa5KMa5DiBEPwV2zAa5KALbziKyAjtmg1wUgW1nEVmBHbNBLorAtrOIrMCO2SAXRWDbWURWYMdskItirOsAQvTTmFg8WeE6xEAEuShqXAcQop8UAf0jxiAXRSCbWUTeENcBBkKKQojiCuS4DXJRBLKZReQFsigCGRpg2bAzmsrpXOc6hxD90USVhlWuY/RbYItiiOqoAka5ziFEf4xkQ4frDAMR5E2PdtcBhBiAQI7bIBfFFtcBhBgAKYoia3UdQIgB2OQ6wEAEuShWuw4gRD99iJfe7DrEQAS5KBpdBxCinwI7ZoNcFMH7jElEXWDHbJCLIrDtLCIrsGM2yEUR2HYWkRXYMRvkoghsO4vICuyYlaIQongCO2aDWxReej2w0XUMIfpBNj0cecN1ACH64XXXAQYq6EWxxHUAIXL0Nl56jesQAxX0oljsOoAQOQr0m1rQiyLQT76IlEC/qQW9KF5F/opUBEOg39SCXRReug14zXUMIXIgReFYoFeAiIRA78iEcBRFoLf9RCQE/s0sDEXxF9cBhPAR+DEa/KLw0q8C/3EdQ4g+POY6wGAFvyiMwK8IEVpv4qUD/xvEUhRCFNajrgPkQ1iK4jmgyXUIIXoQijexcBSF+cLSp1zHECLLWuAF1yHyIRxFYYRiiidC5XG8dCCPDJYtTEXxOBCKlSJCIzRvXuEpCi+9DrOvQohSsAF40nWIfAlPURi3uw4ghDUPL93iOkS+hK0oHiHAXzcmQuVm1wHyKVxF4aXbgbmuY4jIW4iXfsV1iHwKV1EYdxDQI0aL0LjFdYB8C19ReOlVwALXMURkfQA85DpEvoWvKIzQNboIjDvtFyqFSjiLwkv/ngB/NboIrE7gNtchCiGcRWHMcR1ARM7DeOmU6xCFEOai+AXwpusQIjI6gMtchyiU8BaF+ag0tCtOlJxfhOF7J3oT3qIw5iPfqSkKrxXwXIcopHAXhZfWwHddxxCh9xO89ErXIQop3EUB4KWfAZ5xHUOE1nog4TpEoYW/KIw4oF2HEKF0HV76Q9chCi0aReGllwAPuo4hQmclcJPrEMUQjaIwvgmEvvlFUZ2Pl251HaIYolMUXroR+IbrGCI07sVLh+KLc3MRnaIA8NL3EpJvRRZORe5Np8J1AAdmAYcC27kOUiyxG5upHaYoV1BRBovPq2Fdq+bU+RtJrdfERioe/EIV21Wqbre9e+lmrvrzZgAuO2woM/YZCsCSVR3MXNBK6xbNsbsN4abpw1Cq++1DalYUdmBmitaMAiK7CfKHGVUsnV3D4vNqAEg838a0XSpYdmEN03apIPF89z94XNeq+eEf2/jrudW8eG41P/xjGx+2mg+Pzk+2Mvf44Sy7sIZl6zp4cnlkvgLkvihtcnSJXlGAbIIAC95oZ8beQwCYsfcQfvNG9xf6U8vbOXJSBaMqFdtVKo6cVMGTy9tpbO6kqQ0O2qkCpRRn7jWU37weiaJoBL7uOoQL0SwKYxawznWIYlAKjrp3I/vf0cIdS8xmxOqWTsbVmtU/tkaxuqWz2+1WNncyoW7rENlpRBkrmztZ2azZaYTKOF+xsjkSv6YSuU2OLlHcR2F46Ua8utMwxwMpdx2nkJ4/q5r6EWW8v6GTI+/dyB5jtn1/UEoRnd0LA/ajKG5ydInyjAK89NPAJa5jFFr9CLOad6gu4+Q9KnhxZQc71pTR2GxmEY3NnexQ3X0o1NeW8U5660zj3aZO6mvLqK9VvNukM87X1NeGumkex/x2b2RFuygAvPQc4G7XMQplw2ZNc5v+6PTT/+5g6g7lnLB7BXe/sgWAu1/ZwolTuk8uj55cwdMr2vmwVfNhq+bpFe0cPbmCcbVljBgGi95tR2vNPX/fzIl7hHZy+jpwGl66+7ZZhIR27fbTLGAKcJDrIPm2eoPm5Ac2AtDeCadPHcL0yRV8anwZp8xv5ecvb2FineLBL1YBsHhVB7ct3szPTqhkVKXi+58ZxqfmmuPYXP6ZYYyyH6He0lDJzN+00tquOWZyBcdMDuVQWg+cgJduch3ENaV1JHZC+fPqxmK+u6LedRRREjqAY+3maeTJpkcXL/0ecBKwyXESURq+IyWxlRRFJi+9GJiJ+TZlEV1z8dI3uA5RSqQosnnpB4DzkO+viKpfArNdhyg1UhQ98dI/B77mOoYouvnAjKh/wtETKYreeOlbMN9hIaLhUeB0vHSH6yClSIqiL176RszMQjZDwm0+8AW89BbXQUqVFIUfL30zZp+FTEfD6T7gS1ISfZOiyIWX/hlwJrDZdRSRV7dh9knI5oYP+YWr/vDqPg38GtjRdRQxKO3At/DSP3EdJCikKPrLq5sALAD2dR1FDMg64BS89LOugwSJbHr0l5d+B/NVevL1/8HzD+AAKYn+kxnFYHh13wOuBEL9N9Yh8RjwZbx0s+sgQSQzisHw0lcDJwMy+ErbNcBJUhIDJzOKfPDqJgN3Aoe5jiK28S5wHl76CddBgk5mFPngpZcDRwAXARudZhFd7gSmSknkh8wo8s3MLn6B2eEpik9mEQUgM4p8M7OLw5HZhQsyiygQmVEUkpldzMVslojCeRuYLQVROFIUxeDVHYfZ8/4J11FCZi1wNXALXrr7oc5E3khRFItXVwZ8GbgCiLkNE3gbgDnAj+WLb4tDiqLYvLqhmG9QugzY3nGaoNkC3AFciZde7TpMlEhRuOLV1QAXYw6YHJkjqw9QO/AAcDleeoXrMFEkReGaV1cJnAZcAOzvOE2pWYnZGXyHPQq9cESKopR4dQdgCuNUYLjjNC49C9wKLMBLR+Iw6aVOiqIUeXWjgLMx+zJ2dZymWNLAXcCteOk3HGcRWaQoSplXpzCbIyfYf3u7DZR3KzF/1fko8Hv5iLN0SVEEiVe3M6Ywjsf8EtdQp3kG5mW6ysFLL3EdRuRGiiKovLpaYDrwGeCTmNlGpdNM3XVijga+GFgIJO0X/4iAkaIIC6+uHNgTs6myP8Uvj8xSWGL/LcVLbyjS/YsCkqIIM1Mek4HxwLg+/q+m72/pagNWA6uAxh7+bwSWSymElxSFMMyvmA8BKjCl0Q60y8eTAqQohBA5kO+jEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+JKiEEL4kqIQQviSohBC+Pp/EQGLl7I4uJMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(y.value_counts(), autopct=\"%0.2f\")\n",
    "plt.title(\"Distribution of classes after oversampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Classifier with tuned parameters**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters used:** class_weight = \"balanced\", C = 2e-4  \n",
    "We made use of the regularization parameter C. The lower value the value of C, higher the regularization. Thus, we set our C value to be very low , 2e-4. We observed that if we increased C, then the model was performing poorly. We also set our class_weight=balanced. This is so that sklearn automatically gives a certain class its weightage depending on the frequency of the class occurring, and there is no bias to a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8442235540612971\n",
      "0.7671957671957672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(class_weight = \"balanced\", C = 2e-4)\n",
    "model.fit(x_train_st, y_train)\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_train, model.predict(x_train_st)))\n",
    "print(sklearn.metrics.f1_score(y_test, model.predict(x_test_st)))\n",
    "\n",
    "predictions = pd.DataFrame(model.predict(final_test_st))\n",
    "#pd.concat([predictions,final_test_id],axis = 1).to_csv(\"Logistic_with_tuning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting Classifier**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters used:** n_estimators = 600, learning_rate = 0.25, max_depth = 6  \n",
    "We first ran the model with the default value of n_estimators = 100. Due to Gradient Boost's robustness to over-fitting, it typically performs better with a larger number. Thus, to find the sweet spot, we plotted a graph between the F1 score and n_estimators. From the graph, we see that there is a slight peak at n_estimators = 600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9745944596955328\n",
      "0.77959927140255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators = 600, learning_rate = 0.25, max_depth = 6)\n",
    "gbc.fit(x_train_st, y_train)\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_train, gbc.predict(x_train_st)))\n",
    "print(sklearn.metrics.f1_score(y_test, gbc.predict(x_test_st)))\n",
    "predictions = pd.DataFrame(gbc.predict(final_test_st))\n",
    "#pd.concat([predictions,final_test_id],axis = 1).to_csv(\"gradient_boost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model selected (Best performing) : Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters used:** n_estimators = 500, max_depth = 1000, criterion = 'log_loss'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first ran the model with the default value of n_estimators = 100. We then plotted a graph between the F1 score and n_estimators. From the graph, we observed that the accuracy of the model generally remained constant as we increased the number of estimators.  Thus, we finalized n_estimators = 500.\n",
    "\n",
    "Moving on, we adjusted the max_features parameter, by manually trying the three options of {“sqrt”, “log2”, None}. We observed that sqrt gave us the best results.\n",
    "\n",
    "Additionally, we tuned the max_depth and the criterion parameters. Similar to n_estimators, we plotted a graph between the F1 score and max_depth. From the graph, we observe a slight peak at max_depth = 1000.\n",
    "\n",
    "For criterion we tried the three parameters manually, and found that the resulting accuracies were quite similar, but criterion = ‘log_loss’ gave us the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997123015873016\n",
      "0.8107109879963067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et= ExtraTreesClassifier(n_estimators = 500, max_depth = 1000, criterion = 'log_loss')\n",
    "et.fit(x_train_st, y_train)\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_train, et.predict(x_train_st)))\n",
    "print(sklearn.metrics.f1_score(y_test, et.predict(x_test_st)))\n",
    "\n",
    "predictions = pd.DataFrame(et.predict(final_test_st))\n",
    "#pd.concat([predictions,final_test_id],axis = 1).to_csv(\"extra_trees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voting Classifier with Extra Trees, Logistic regression and Gradient Boost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters used:** voting = \"soft\", n_jobs = -1  \n",
    "For voting classifier, we chose extra trees, logistic and gradient boosting classifiers, as these three were our top performing models. However, the total accuracy we got was still lesser than our best model, which was extra trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880103051922315\n",
      "0.7949412827461608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "et = ExtraTreesClassifier()\n",
    "logistic = LogisticRegression()\n",
    "xgb = GradientBoostingClassifier()\n",
    "estimators = [(\"extra trees\", et), (\"logistic regression\", logistic), (\"gradient boost\", xgb)]\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting = \"soft\", n_jobs = -1)\n",
    "ensemble.fit(x_train_st, y_train)\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_train, ensemble.predict(x_train_st)))\n",
    "print(sklearn.metrics.f1_score(y_test, ensemble.predict(x_test_st)))\n",
    "\n",
    "predictions = pd.DataFrame(ensemble.predict(final_test_st))\n",
    "#pd.concat([predictions,final_test_id],axis = 1).to_csv(\"voting.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging Classifier** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters used: n_estimators = 10**   \n",
    "For bagging classifier, we chose extra trees with n_estimators = 10 because we felt that extra trees classifier was very rigid in training the data. However, bagging classifier also underperformed compared to extra tree classifier. We  also plotted a graph between F1 score and n_estimators. We see a slight decrease in the F1 score as we apply n_estimators of more than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9941532058269745\n",
      "0.815018315018315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier(base_estimator = ExtraTreesClassifier(),n_estimators = 10)\n",
    "bag.fit(x_train_st, y_train)\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_train, bag.predict(x_train_st)))\n",
    "print(sklearn.metrics.f1_score(y_test, bag.predict(x_test_st)))\n",
    "\n",
    "predictions = pd.DataFrame(ensemble.predict(final_test_st))\n",
    "#pd.concat([predictions,final_test_id],axis = 1).to_csv(\"bagging.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters used: max_depth = 25**    \n",
    "By plotting a graph between F1 score and max_depth, we found that the F1 score was increasing as the max_depth increased.   Hence, we chose max_depth = 25 considering the time required to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7894094928346376\n",
      "0.7126654064272211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf= RandomForestClassifier(max_depth = 25)\n",
    "rf.fit(x_train_st, y_train)\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_train, rf.predict(x_train_st)))\n",
    "print(sklearn.metrics.f1_score(y_test, rf.predict(x_test_st)))\n",
    "\n",
    "predictions = pd.DataFrame(rf.predict(final_test_st))\n",
    "#pd.concat([predictions,final_test_id],axis = 1).to_csv(\"random_forest.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
